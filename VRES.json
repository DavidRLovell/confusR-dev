[
  {"id":"3blue1brown_medical_2020","abstract":"About Likelihood Ratios, also sometimes called Bayes Factors*.\nViewer-supported: https://3b1b.co/bayes-factor-thanks\nHome page: https://www.3blue1brown.com\n\nThe book by my friend Matt Cook about paradoxes mentioned at the end:\nhttps://amzn.to/3aBrEzg\n\nOn the topic, I can't help also mentioning another paradox book I'm rather fond of by Bunch:\nhttps://amzn.to/3mBDSKE\n\nAnother video on Bayes' theorem:\nhttps://youtu.be/HZGCoVF3YvM\n\n*As mentioned in the on-screen note at the end, while the terms \"Bayes Factor\" and \"Likelihood Ratio\" refer to the same ratio in this setting, where Bayes rule is used on the probability of an event with only two possible outcomes (you either have the disease or you don't), they do take on divergent meanings in more general contexts.  Namely, if you have a continuous parameter you are trying to estimate, the two terms reflect two alternate approaches you can use in comparing hypotheses.  In fact, some people take the phrase \"Bayes factor\" to _specifically_ refer to its use in this more continuous context.\n\nIf you want more details, Wikipedia actually has a really nice example discussing the difference:\nhttps://en.wikipedia.org/wiki/Bayes_f...\n\nThis post has some nice discussion of the distinction:\nhttps://stats.stackexchange.com/quest...\n\n------------------ \n \nThese animations are largely made using manim, a scrappy open source python library:  https://github.com/3b1b/manim \n \nIf you want to check it out, I feel compelled to warn you that it's not the most well-documented tool, and it has many other quirks you might expect in a library someone wrote with only their own use in mind. \n \nMusic by Vincent Rubinetti. \nDownload the music on Bandcamp: \nhttps://vincerubinetti.bandcamp.com/a... \n \nStream the music on Spotify: \nhttps://open.spotify.com/album/1dVyjw... \n \nIf you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles/cc, then \"add subtitles/cc\".  I really appreciate those who do this, as it helps make the lessons accessible to more people. \n \n------------------ \n \n3blue1brown is a channel about animating math, in all senses of the word animate.  And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http://3b1b.co/subscribe \n \nVarious social media stuffs: \nWebsite: https://www.3blue1brown.com \nTwitter: https://twitter.com/3blue1brown \nReddit: https://www.reddit.com/r/3blue1brown \nInstagram: https://www.instagram.com/3blue1brown... \nPatreon: https://patreon.com/3blue1brown \nFacebook: https://www.facebook.com/3blue1brown","accessed":{"date-parts":[[2021,1,18]]},"dimensions":"21:13","director":[{"literal":"3Blue1Brown"}],"issued":{"date-parts":[[2020,12,22]]},"source":"YouTube","title":"The medical test paradox: Can redesigning Bayes rule help?","title-short":"The medical test paradox","type":"motion_picture","URL":"https://www.youtube.com/watch?v=lG4VkPoG3ko"},
  {"id":"adams_improving_2000","abstract":"In this note we use examples from the literature to illustrate some poor practices in assessing the performance of supervised classification rules, and we suggest guidelines for better methodology. We also describe a new assessment criterion that is suitable for the needs of many practical problems.","accessed":{"date-parts":[[2020,11,14]]},"author":[{"family":"Adams","given":"N. M."},{"family":"Hand","given":"D. J."}],"container-title":"Neural Computation","container-title-short":"Neural Computation","DOI":"10.1162/089976600300015808","ISSN":"0899-7667, 1530-888X","issue":"2","issued":{"date-parts":[[2000,2,1]]},"language":"en","page":"305-311","source":"DOI.org (Crossref)","title":"Improving the Practice of Classifier Performance Assessment","type":"article-journal","URL":"https://www.mitpressjournals.org/doi/abs/10.1162/089976600300015808","volume":"12"},
  {"id":"allwein_reducing_2000","accessed":{"date-parts":[[2021,1,18]]},"author":[{"family":"Allwein","given":"Erin L."},{"family":"Schapire","given":"Robert E."},{"family":"Singer","given":"Yoram"}],"container-title":"Journal of Machine Learning Research","ISSN":"ISSN 1533-7928","issue":"Dec","issued":{"date-parts":[[2000]]},"page":"113-141","source":"www.jmlr.org","title":"Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers","title-short":"Reducing Multiclass to Binary","type":"article-journal","URL":"https://www.jmlr.org/papers/v1/allwein00a.html","volume":"1"},
  {"id":"alsallakh_visual_2014","abstract":"Multi-class classifiers often compute scores for the classification samples describing probabilities to belong to different classes. In order to improve the performance of such classifiers, machine learning experts need to analyze classification results for a large number of labeled samples to find possible reasons for incorrect classification. Confusion matrices are widely used for this purpose. However, they provide no information about classification scores and features computed for the samples. We propose a set of integrated visual methods for analyzing the performance of probabilistic classifiers. Our methods provide insight into different aspects of the classification results for a large number of samples. One visualization emphasizes at which probabilities these samples were classified and how these probabilities correlate with classification error in terms of false positives and false negatives. Another view emphasizes the features of these samples and ranks them by their separation power between selected true and false classifications. We demonstrate the insight gained using our technique in a benchmarking classification dataset, and show how it enables improving classification performance by interactively defining and evaluating post-classification rules.","author":[{"family":"Alsallakh","given":"Bilal"},{"family":"Hanbury","given":"Allan"},{"family":"Hauser","given":"Helwig"},{"family":"Miksch","given":"Silvia"},{"family":"Rauber","given":"Andreas"}],"container-title":"IEEE Transactions on Visualization and Computer Graphics","DOI":"10.1109/TVCG.2014.2346660","ISSN":"1941-0506","issue":"12","issued":{"date-parts":[[2014,12]]},"page":"1703-1712","source":"IEEE Xplore","title":"Visual Methods for Analyzing Probabilistic Classification Data","type":"article-journal","volume":"20"},
  {"id":"buja_loss_2005","abstract":"What are the natural loss functions or fitting criteria for binary class probability estimation? This question has a simple answer: so-called “proper scoring rules”, that is, functions that score probability estimates in view of data in a Fisher-consistent manner. Proper scoring rules comprise most loss functions currently in use: log-loss, squared error loss, boosting loss, and as limiting cases cost-weighted misclassification losses. Proper scoring rules have a rich structure: • Every proper scoring rules is a mixture (limit of sums) of cost-weighted misclassification losses. The mixture is specified by a weight function (or measure) that describes which misclassification cost weights are most emphasized by the proper scoring rule. • Proper scoring rules permit Fisher scoring and Iteratively Reweighted LS algorithms for model fitting. The weights are derived from a link function and the above weight function. • Proper scoring rules are in a 1-1 correspondence with information measures for tree-based classification.","author":[{"family":"Buja","given":"Andreas"},{"family":"Stuetzle","given":"Werner"},{"family":"Shen","given":"Yi"}],"issued":{"date-parts":[[2005]]},"source":"CiteSeer","title":"Loss Functions for Binary Class Probability Estimation and Classification: Structure and Applications,” manuscript, available at www-stat.wharton.upenn.edu/ buja","title-short":"Loss Functions for Binary Class Probability Estimation and Classification","type":"book","URL":"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.5203"},
  {"id":"caelen_bayesian_2017","abstract":"We propose a way to infer distributions of any performance indicator computed from the confusion matrix. This allows us to evaluate the variability of an indicator and to assess the importance of an observed difference between two performance indicators. We will assume that the values in a confusion matrix are observations coming from a multinomial distribution. Our method is based on a Bayesian approach in which the unknown parameters of the multinomial probability function themselves are assumed to be generated from a random vector. We will show that these unknown parameters follow a Dirichlet distribution. Thanks to the Bayesian approach, we also benefit from an elegant way of injecting prior knowledge into the distributions. Experiments are done on real and synthetic data sets and assess our method’s ability to construct accurate distributions.","accessed":{"date-parts":[[2021,1,5]]},"author":[{"family":"Caelen","given":"Olivier"}],"container-title":"Annals of Mathematics and Artificial Intelligence","container-title-short":"Ann Math Artif Intell","DOI":"10.1007/s10472-017-9564-8","ISSN":"1573-7470","issue":"3","issued":{"date-parts":[[2017,12,1]]},"language":"en","page":"429-450","source":"Springer Link","title":"A Bayesian interpretation of the confusion matrix","type":"article-journal","URL":"https://doi.org/10.1007/s10472-017-9564-8","volume":"81"},
  {"id":"diamond_wizard_1999","abstract":"‘Tis with our judgements as our watches, none go just alike, yet each believes\nhis own.Alexander Pope","accessed":{"date-parts":[[2021,5,24]]},"author":[{"family":"Diamond","given":"George A."}],"container-title":"Mayo Clinic Proceedings","container-title-short":"Mayo Clinic Proceedings","DOI":"10.4065/74.11.1179","ISSN":"0025-6196, 1942-5546","issue":"11","issued":{"date-parts":[[1999,11,1]]},"language":"English","page":"1179-1182","publisher":"Elsevier","source":"www.mayoclinicproceedings.org","title":"The Wizard of Odds: Bayes' Theorem and Diagnostic Testing","title-short":"The Wizard of Odds","type":"article-journal","URL":"https://www.mayoclinicproceedings.org/article/S0025-6196(11)65108-2/abstract","volume":"74"},
  {"id":"filzmoser_discriminant_2012","accessed":{"date-parts":[[2021,2,2]]},"author":[{"family":"Filzmoser","given":"Peter"},{"family":"Hron","given":"Karel"},{"family":"Templ","given":"Matthias"}],"container-title":"Computational Statistics","container-title-short":"Comput Stat","DOI":"10.1007/s00180-011-0279-8","ISSN":"0943-4062, 1613-9658","issue":"4","issued":{"date-parts":[[2012,12]]},"language":"en","page":"585-604","source":"DOI.org (Crossref)","title":"Discriminant analysis for compositional data and robust parameter estimation","type":"article-journal","URL":"http://link.springer.com/10.1007/s00180-011-0279-8","volume":"27"},
  {"id":"glas_diagnostic_2003","abstract":"Diagnostic testing can be used to discriminate subjects with a target disorder from subjects without it. Several indicators of diagnostic performance have been proposed, such as sensitivity and specificity. Using paired indicators can be a disadvantage in comparing the performance of competing tests, especially if one test does not outperform the other on both indicators. Here we propose the use of the odds ratio as a single indicator of diagnostic performance. The diagnostic odds ratio is closely linked to existing indicators, it facilitates formal meta-analysis of studies on diagnostic test performance, and it is derived from logistic models, which allow for the inclusion of additional variables to correct for heterogeneity. A disadvantage is the impossibility of weighing the true positive and false positive rate separately. In this article the application of the diagnostic odds ratio in test evaluation is illustrated.","accessed":{"date-parts":[[2021,6,5]]},"author":[{"family":"Glas","given":"Afina S."},{"family":"Lijmer","given":"Jeroen G."},{"family":"Prins","given":"Martin H."},{"family":"Bonsel","given":"Gouke J."},{"family":"Bossuyt","given":"Patrick M. M."}],"container-title":"Journal of Clinical Epidemiology","container-title-short":"Journal of Clinical Epidemiology","DOI":"10.1016/S0895-4356(03)00177-X","ISSN":"0895-4356","issue":"11","issued":{"date-parts":[[2003,11,1]]},"language":"en","page":"1129-1135","source":"ScienceDirect","title":"The diagnostic odds ratio: a single indicator of test performance","title-short":"The diagnostic odds ratio","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S089543560300177X","volume":"56"},
  {"id":"hand_classifier_2006","abstract":"A great many tools have been developed for supervised classification, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm.","accessed":{"date-parts":[[2021,1,13]]},"author":[{"family":"Hand","given":"David J."}],"container-title":"Statistical Science","container-title-short":"Statist. Sci.","DOI":"10.1214/088342306000000060","ISSN":"0883-4237, 2168-8745","issue":"1","issued":{"date-parts":[[2006,2]]},"language":"en","page":"1-14","publisher":"Institute of Mathematical Statistics","source":"Project Euclid","title":"Classifier Technology and the Illusion of Progress","type":"article-journal","URL":"https://projecteuclid.org/euclid.ss/1149600839","volume":"21"},
  {"id":"hand_measuring_2009","abstract":"The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p≠P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.","accessed":{"date-parts":[[2021,1,13]]},"author":[{"family":"Hand","given":"David J."}],"container-title":"Machine Learning","container-title-short":"Mach Learn","DOI":"10.1007/s10994-009-5119-5","ISSN":"1573-0565","issue":"1","issued":{"date-parts":[[2009,10,1]]},"language":"en","page":"103-123","source":"Springer Link","title":"Measuring classifier performance: a coherent alternative to the area under the ROC curve","title-short":"Measuring classifier performance","type":"article-journal","URL":"https://doi.org/10.1007/s10994-009-5119-5","volume":"77"},
  {"id":"hand_simple_nodate","abstract":"The area under the ROC curve, or the equivalent Gini index, is a widely used measure of performance of supervised classiﬁcation rules. It has the attractive property that it side-steps the need to specify the costs of the different kinds of misclassiﬁcation. However, the simple form is only applicable to the case of two classes. We extend the deﬁnition to the case of more than two classes by averaging pairwise comparisons. This measure reduces to the standard form in the two class case. We compare its properties with the standard measure of proportion correct and an alternative deﬁnition of proportion correct based on pairwise comparison of classes for a simple artiﬁcial case and illustrate its application on eight data sets. On the data sets we examined, the measures produced similar, but not identical results, reﬂecting the different aspects of performance that they were measuring. Like the area under the ROC curve, the measure we propose is useful in those many situations where it is impossible to give costs for the different kinds of misclassiﬁcation.","author":[{"family":"Hand","given":"David J"}],"language":"en","page":"16","source":"Zotero","title":"A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems","type":"article-journal"},
  {"id":"harrell_classification_2017","abstract":"It is important to distinguish prediction and classification. In many decisionmaking contexts, classification represents a premature decision, because classification combines prediction and decision making and usurps the decision maker in specifying costs of wrong decisions.","accessed":{"date-parts":[[2021,1,7]]},"author":[{"family":"Harrell","given":"Frank"}],"container-title":"Statistical Thinking","issued":{"date-parts":[[2017,1,15]]},"language":"en-us","title":"Classification vs. Prediction","type":"webpage","URL":"https://www.fharrell.com/post/classification/"},
  {"id":"hastie_classification_1998","abstract":"We discuss a strategy for polychotomous classification that involves coupling the estimating class probabilities for each pair of classes, and estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated data sets. Classifiers used include linear discriminants, nearest neighbors, adaptive nonlinear methods and the support vector machine.","accessed":{"date-parts":[[2021,1,18]]},"author":[{"family":"Hastie","given":"Trevor"},{"family":"Tibshirani","given":"Robert"}],"container-title":"Annals of Statistics","container-title-short":"Ann. Statist.","DOI":"10.1214/aos/1028144844","ISSN":"0090-5364, 2168-8966","issue":"2","issued":{"date-parts":[[1998,4]]},"language":"en","page":"451-471","publisher":"Institute of Mathematical Statistics","source":"Project Euclid","title":"Classification by pairwise coupling","type":"article-journal","URL":"http://projecteuclid.org/euclid.aos/1028144844","volume":"26"},
  {"id":"johnson_using_2017","abstract":"In the context of diagnostic testing, concepts such as sensitivity, specificity, predictive values, likelihood ratios and more are all interconnected, but precisely how can be confusing to the nonstatistician. This paper presents a graphical explanation. Bayes’ rule, or theorem, ties several of these concepts together and should have a role in careful diagnostic thinking. It too can be understood in terms of the “two-by-two diagram” presented here.","accessed":{"date-parts":[[2021,5,24]]},"author":[{"family":"Johnson","given":"Kevin M."}],"container-title":"Diagnosis","DOI":"10.1515/dx-2017-0011","ISSN":"2194-802X","issue":"3","issued":{"date-parts":[[2017,9,1]]},"language":"en","page":"159-167","publisher":"De Gruyter","section":"Diagnosis","source":"www.degruyter.com","title":"Using Bayes’ rule in diagnostic testing: a graphical explanation","title-short":"Using Bayes’ rule in diagnostic testing","type":"article-journal","URL":"https://www.degruyter.com/document/doi/10.1515/dx-2017-0011/html","volume":"4"},
  {"id":"kass_bayes_1995","abstract":"In a 1935 paper and in his book \"Theory of Probability,\" H. Jeffreys developed a method for quantifying the evidence in favor of a scientific theory. The uses of Bayes factors in five scientific applications are discussed.","accessed":{"date-parts":[[2021,1,5]]},"author":[{"family":"Kass","given":"Robert E."},{"family":"Raftery","given":"Adrian E."}],"container-title":"Journal of the American Statistical Association","event-place":"Alexandria, United Kingdom","ISSN":"01621459","issue":"430","issued":{"date-parts":[[1995,6]]},"language":"English","number-of-pages":"23","page":"773","publisher":"Taylor & Francis Ltd.","publisher-place":"Alexandria, United Kingdom","source":"ProQuest","title":"Bayes factors","type":"article-journal","URL":"https://www.proquest.com/docview/274636169/abstract/2C85B3EC3184EE8PQ/1","volume":"90"},
  {"id":"landgrebe_approximating_2007","accessed":{"date-parts":[[2021,1,6]]},"author":[{"family":"Landgrebe","given":"Thomas C.W."},{"family":"Duin","given":"Robert P.W."}],"container-title":"Pattern Recognition Letters","DOI":"10.1016/j.patrec.2007.05.001","ISSN":"01678655","issue":"13","issued":{"date-parts":[[2007,10]]},"language":"en","page":"1747-1758","source":"Crossref","title":"Approximating the multiclass ROC by pairwise analysis","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S016786550700150X","volume":"28"},
  {"id":"lovell_counts_2020","abstract":"Abstract\n            Thanks to sequencing technology, modern molecular bioscience datasets are often compositions of counts, e.g. counts of amplicons, mRNAs, etc. While there is growing appreciation that compositional data need special analysis and interpretation, less well understood is the discrete nature of these count compositions (or, as we call them, lattice compositions) and the impact this has on statistical analysis, particularly log-ratio analysis (LRA) of pairwise association. While LRA methods are scale-invariant, count compositional data are not; consequently, the conclusions we draw from LRA of lattice compositions depend on the scale of counts involved. We know that additive variation affects the relative abundance of small counts more than large counts; here we show that additive (quantization) variation comes from the discrete nature of count data itself, as well as (biological) variation in the system under study and (technical) variation from measurement and analysis processes. Variation due to quantization is inevitable, but its impact on conclusions depends on the underlying scale and distribution of counts. We illustrate the different distributions of real molecular bioscience data from different experimental settings to show why it is vital to understand the distributional characteristics of count data before applying and drawing conclusions from compositional data analysis methods.","accessed":{"date-parts":[[2021,1,19]]},"author":[{"family":"Lovell","given":"David R"},{"family":"Chua","given":"Xin-Yi"},{"family":"McGrath","given":"Annette"}],"container-title":"NAR Genomics and Bioinformatics","DOI":"10.1093/nargab/lqaa040","ISSN":"2631-9268","issue":"2","issued":{"date-parts":[[2020,6,1]]},"language":"en","page":"lqaa040","source":"DOI.org (Crossref)","title":"Counts: an outstanding challenge for log-ratio analysis of compositional data in the molecular biosciences","title-short":"Counts","type":"article-journal","URL":"https://academic.oup.com/nargab/article/doi/10.1093/nargab/lqaa040/5859926","volume":"2"},
  {"id":"luana_micallef_explaining_2012","abstract":"Explains the classic Bayesian mammography problem using a visualization (an area-proportional Euler diagram with glyphs) rather than using Bayes' theorem. More at:\nhttp://www.aviz.fr/bayes\nhttp://www.eulerdiagrams.org/eulerGlyphs","accessed":{"date-parts":[[2021,6,13]]},"dimensions":"2:20","director":[{"literal":"Luana Micallef"}],"issued":{"date-parts":[[2012,11,8]]},"source":"YouTube","title":"Explaining Bayesian Problems Using Visualizations","type":"motion_picture","URL":"https://www.youtube.com/watch?v=D8VZqxcu0I0"},
  {"id":"merkle_choosing_2013","abstract":"Strictly proper scoring rules, including the Brier score and the logarithmic score, are standard metrics by which probability forecasters are assessed and compared. Researchers often find that one's choice of strictly proper scoring rule has minimal impact on one's conclusions, but this conclusion is typically drawn from a small set of popular rules. In the context of forecasting world events, we use a recently proposed family of proper scoring rules to study the properties of a wide variety of strictly proper rules. The results indicate that conclusions vary greatly across different scoring rules, so that one's choice of scoring rule should be informed by the forecasting domain. We then describe strategies for choosing a scoring rule that meets the needs of the forecast consumer, considering three unique families of proper scoring rules.","accessed":{"date-parts":[[2021,1,13]]},"author":[{"family":"Merkle","given":"Edgar C."},{"family":"Steyvers","given":"Mark"}],"container-title":"Decision Analysis","container-title-short":"Decision Analysis","DOI":"10.1287/deca.2013.0280","ISSN":"1545-8490","issue":"4","issued":{"date-parts":[[2013,12,1]]},"page":"292-304","publisher":"INFORMS","source":"pubsonline.informs.org (Atypon)","title":"Choosing a Strictly Proper Scoring Rule","type":"article-journal","URL":"https://pubsonline.informs.org/doi/abs/10.1287/deca.2013.0280","volume":"10"},
  {"id":"murphy_machine_2012","author":[{"family":"Murphy","given":"Kevin P."}],"call-number":"Q325.5 .M87 2012","collection-title":"Adaptive computation and machine learning series","event-place":"Cambridge, MA","ISBN":"978-0-262-01802-9","issued":{"date-parts":[[2012]]},"number-of-pages":"1067","publisher":"MIT Press","publisher-place":"Cambridge, MA","source":"Library of Congress ISBN","title":"Machine learning: a probabilistic perspective","title-short":"Machine learning","type":"book"},
  {"id":"noauthor_hmeasurenet_nodate","abstract":"Tools and theory on measuring classification performance.","accessed":{"date-parts":[[2021,1,13]]},"container-title":"hmeasure.net","language":"en","title":"hmeasure.net","type":"webpage","URL":"http://www.hmeasure.net/"},
  {"id":"noauthor_intrinsic_1996","accessed":{"date-parts":[[2021,5,27]]},"container-title":"Journal of the American Statistical Association","event-place":"Alexandria, United Kingdom","ISSN":"01621459","issue":"433","issued":{"date-parts":[[1996,3]]},"language":"English","number-of-pages":"14","page":"109","publisher":"Taylor & Francis Ltd.","publisher-place":"Alexandria, United Kingdom","source":"ProQuest","title":"The intrinsic Bayes factor for model selection and predictio","type":"article-journal","URL":"https://www.proquest.com/docview/274816990/citation/DE19DAC180644C61PQ/1","volume":"91"},
  {"id":"noauthor_machine_nodate","accessed":{"date-parts":[[2021,1,7]]},"container-title":"Cross Validated","title":"machine learning - Why is accuracy not the best measure for assessing classification models?","type":"webpage","URL":"https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models"},
  {"id":"noauthor_performance_2018","abstract":"How to calculate performance for multi-class problems? Learn about micro- and macro-averaged F1-scores as well as a generalization of the AUC here!","accessed":{"date-parts":[[2021,1,5]]},"issued":{"date-parts":[[2018,12,4]]},"language":"en","title":"Performance Measures for Multi-Class Problems","type":"webpage","URL":"https://www.datascienceblog.net/post/machine-learning/performance-measures-multi-class-problems/"},
  {"id":"noauthor_probability_nodate","accessed":{"date-parts":[[2021,1,7]]},"container-title":"Cross Validated","title":"probability - Is accuracy an improper scoring rule in a binary classification setting?","type":"webpage","URL":"https://stats.stackexchange.com/questions/359909/is-accuracy-an-improper-scoring-rule-in-a-binary-classification-setting"},
  {"id":"noauthor_taking_nodate","accessed":{"date-parts":[[2021,1,5]]},"title":"Taking the Confusion out of the Confusion Matrix","type":"webpage","URL":"https://lucdemortier.github.io/articles/16/PerformanceMetrics"},
  {"id":"olivetti_statistical_2014","abstract":"Machine learning techniques are increasingly adopted in computer-aided diagnosis. Evaluation methods for classification results that are based on the study of one or more metrics can be unable to distinguish between cases in which the classifier is discriminating the classes from cases in which it is not. In the binary setting, such circumstances can be encountered when data are unbalanced with respect to the diagnostic groups. Having more healthy controls than pathological subjects, datasets meant for diagnosis frequently show a certain degree of unbalancedness. In this work, we propose to recast the evaluation of classification results as a test of statistical independence between the predicted and the actual diagnostic groups. We address the problem within the Bayesian hypothesis testing framework. Different from the standard metrics, the proposed method is able to handle unbalanced data and takes into account the size of the available data. We show experimental evidence of the efficacy of the approach both on simulated data and on real data about the diagnosis of the Attention Deficit Hyperactivity Disorder (ADHD).","accessed":{"date-parts":[[2021,1,18]]},"author":[{"family":"Olivetti","given":"Emanuele"},{"family":"Greiner","given":"Susanne"},{"family":"Avesani","given":"Paolo"}],"container-title":"Brain Informatics","container-title-short":"Brain Inform","DOI":"10.1007/s40708-014-0007-6","ISSN":"2198-4018","issue":"1","issued":{"date-parts":[[2014,12,11]]},"page":"13-19","PMCID":"PMC4883157","PMID":"27747500","source":"PubMed Central","title":"Statistical independence for the evaluation of classifier-based diagnosis","type":"article-journal","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4883157/","volume":"2"},
  {"id":"pawlowsky-glahn_compositional_2011","abstract":"It is difficult to imagine that the statistical analysis of compositional data has been a major issue of concern for more than 100 years. It is even more difficult to realize that so many statisticians and users of statistics are unaware of the particular problems affecting compositional data, as well as their solutions. The issue of ``spurious correlation'', as the situation was phrased by Karl Pearson back in 1897, affects all data that measures parts of some whole, such as percentages, proportions, ppm and ppb. Such measurements are present in all fields of science, ranging from geology, biology, environmental sciences, forensic sciences, medicine and hydrology. This book presents the history and development of compositional data analysis along with Aitchison's log-ratio approach. Compositional Data Analysis describes the state of the art both in theoretical fields as well as applications in the different fields of science. Key Features: Applied scientists working on compositional data analysis in any field of science, both in academia and professionals will benefit from this book, along with graduate students in any field of science working with compositional data.","accessed":{"date-parts":[[2013,3,18]]},"editor":[{"family":"Pawlowsky-Glahn","given":"Vera"},{"family":"Buccianti","given":"Antonella"}],"ISBN":"978-0-470-71135-4 978-1-119-97646-2","issued":{"date-parts":[[2011,7,28]]},"source":"Wiley Online Library","title":"Compositional Data Analysis: Theory and Applications","title-short":"Compositional Data Analysis","type":"book","URL":"http://onlinelibrary.wiley.com/book/10.1002/9781119976462"},
  {"id":"provost_analysis_1997","abstract":"Applications of inductive learning algorithms to realworld data mining problems have shown repeatedly that using accuracy to compare classifiers is not adequate because the underlying assumptions rarely hold. We present a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Introduction  When mining data with inductive methods, we often experiment with a wide variety of learning algorithms, using different algorithm parameters, varying output threshold values, and using different training regimens. Such experimentation yields a large number of classifiers to be evaluated a...","author":[{"family":"Provost","given":"Foster"},{"family":"Fawcett","given":"Tom"}],"container-title":"In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining","issued":{"date-parts":[[1997]]},"page":"43–48","publisher":"AAAI Press","source":"CiteSeer","title":"Analysis and Visualization of Classifier Performance: Comparison under Imprecise Class and Cost Distributions","title-short":"Analysis and Visualization of Classifier Performance","type":"paper-conference"},
  {"id":"shen_loss_2005","author":[{"family":"Shen","given":"Yi"}],"container-title":"Dissertations available from ProQuest","issued":{"date-parts":[[2005,1,1]]},"page":"1-115","title":"Loss functions for binary classification and class probability estimation","type":"article-journal","URL":"https://repository.upenn.edu/dissertations/AAI3179814"},
  {"id":"terrence_fagan_nomogram_1975","abstract":"To the Editor: The interest in Dr. Katz's probability graph (N Engl J Med 291:1115, 1974) causes me to offer a solution to the Bayes's rule in the form of a nomogram (Fig. 1). P(D) is the probability that the patient has the disease before the test. P(D|T) is the probability that the patient has the disease after the test result. P(T|D) is the probability of the test result if the patient has the disease, and P(T|D̄) is the probability of the test result if the patient does not have the disease. With this terminology the usefulness of both positive . . .","accessed":{"date-parts":[[2021,1,7]]},"author":[{"literal":"Terrence Fagan"}],"container-title":"New England Journal of Medicine","DOI":"10.1056/NEJM197507312930513","ISSN":"0028-4793","issue":"5","issued":{"date-parts":[[1975,7,31]]},"note":"_eprint: https://doi.org/10.1056/NEJM197507312930513","page":"257-257","PMID":"1143310","publisher":"Massachusetts Medical Society","source":"Taylor and Francis+NEJM","title":"Nomogram for Bayes's Theorem","type":"article-journal","URL":"https://doi.org/10.1056/NEJM197507312930513","volume":"293"},
  {"id":"tversky_probabilistic_1982","abstract":"To a great extent, the quality and cost of health care are determined by the decisions made by physicians whose ultimate objective is to design and administer a treatment program to improve a patient's condition. Most of the decisions involve many factors, great uncertainty, and difficult value questions.This chapter examines one aspect of how these decisions are made, studying the use of probabilistic reasoning to analyze a particular problem: whether to perform a biopsy on a woman who has a breast mass that might be malignant. Specifically, we shall study how physicians process information about the results of a mammogram, an X-ray test used to diagnose breast cancer. The evidence presented shows that physicians do not manage uncertainty very well, that many physicians make major errors in probabilistic reasoning, and that these errors threaten the quality of medical care.The problemA breast biopsy is not a trivial procedure. The most common type (around 80%) is the excisional biopsy, in which the suspicious mass is removed surgically for microscopic examination and histological diagnosis by a pathologist. Usually the patient is admitted to a hospital and given a full set of preoperative diagnostic tests. The biopsy is almost always done under general anesthesia (with a probability of approximately 2 out of 10,000 of an anesthetic death).","accessed":{"date-parts":[[2020,9,2]]},"author":[{"family":"Eddy","given":"David M."}],"container-title":"Judgment under Uncertainty: Heuristics and Biases","DOI":"10.1017/CBO9780511809477.019","editor":[{"family":"Tversky","given":"Amos"},{"family":"Kahneman","given":"Daniel"},{"family":"Slovic","given":"Paul"}],"event-place":"Cambridge","ISBN":"978-0-521-28414-1","issued":{"date-parts":[[1982]]},"page":"249-267","publisher":"Cambridge University Press","publisher-place":"Cambridge","source":"Cambridge University Press","title":"Probabilistic reasoning in clinical medicine: Problems and opportunities","title-short":"Probabilistic reasoning in clinical medicine","type":"chapter","URL":"https://www.cambridge.org/core/books/judgment-under-uncertainty/probabilistic-reasoning-in-clinical-medicine-problems-and-opportunities/661E12D1ECD669EDB5B410407A4BB570"},
  {"id":"wu_probability_2004","accessed":{"date-parts":[[2021,1,18]]},"author":[{"family":"Wu","given":"Ting-Fan"},{"family":"Lin","given":"Chih-Jen"},{"family":"Weng","given":"Ruby C."}],"container-title":"Journal of Machine Learning Research","ISSN":"ISSN 1533-7928","issue":"Aug","issued":{"date-parts":[[2004]]},"page":"975-1005","source":"www.jmlr.org","title":"Probability Estimates for Multi-class Classification by Pairwise Coupling","type":"article-journal","URL":"https://www.jmlr.org/papers/v5/wu04a.html?907d3908","volume":"5"},
  {"id":"zadrozny_learning_2004","abstract":"Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.","accessed":{"date-parts":[[2021,1,17]]},"author":[{"family":"Zadrozny","given":"Bianca"}],"collection-title":"ICML '04","container-title":"Proceedings of the twenty-first international conference on Machine learning","DOI":"10.1145/1015330.1015425","event-place":"New York, NY, USA","ISBN":"978-1-58113-838-2","issued":{"date-parts":[[2004,7,4]]},"page":"114","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Learning and evaluating classifiers under sample selection bias","type":"paper-conference","URL":"http://doi.org/10.1145/1015330.1015425"},
  {"id":"zadrozny_obtaining_2001-1","abstract":"Accurate, well-calibrated estimates of class membership probabilities are needed in many supervised learning applications, in particular when a cost-sensitive decision must be made about examples with example-dependent costs. This paper presents simple but successful methods for obtaining calibrated probability estimates from decision tree and naive Bayesian classifiers. Using the large and challenging KDD'98 contest dataset as a testbed, we report the results of a detailed experimental comparison of ten methods, according to four evaluation measures. We conclude that binning succeeds in significantly improving naive Bayesian probability estimates, while for improving decision tree probability estimates, we recommend smoothing by -estimation and a new variant of pruning that we call curtailment.","author":[{"family":"Zadrozny","given":"Bianca"},{"family":"Elkan","given":"Charles"}],"container-title":"In Proceedings of the Eighteenth International Conference on Machine Learning","issued":{"date-parts":[[2001]]},"page":"609–616","publisher":"Morgan Kaufmann","source":"CiteSeer","title":"Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers","type":"paper-conference"},
  {"id":"zadrozny_reducing_2001","abstract":"This paper presents a method for obtaining class membership probability estimates for multiclass classification problems by coupling the probability estimates produced by binary classifiers. This is an extension for arbitrary code matrices of a method due to Hastie and Tibshirani for pairwise coupling of probability estimates. Experimental results with Boosted Naive Bayes show that our method produces calibrated class membership probability estimates, while having similar classification accuracy as loss-based decoding, a method for obtaining the most likely class that does not generate probability estimates.","accessed":{"date-parts":[[2021,1,17]]},"author":[{"family":"Zadrozny","given":"Bianca"}],"collection-title":"NIPS'01","container-title":"Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic","event-place":"Cambridge, MA, USA","issued":{"date-parts":[[2001,1,3]]},"page":"1041–1048","publisher":"MIT Press","publisher-place":"Cambridge, MA, USA","source":"ACM Digital Library","title":"Reducing multiclass to binary by coupling probability estimates","type":"paper-conference"},
  {"id":"zadrozny_transforming_2002","abstract":"Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.","accessed":{"date-parts":[[2021,1,17]]},"author":[{"family":"Zadrozny","given":"Bianca"},{"family":"Elkan","given":"Charles"}],"collection-title":"KDD '02","container-title":"Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining","DOI":"10.1145/775047.775151","event-place":"New York, NY, USA","ISBN":"978-1-58113-567-1","issued":{"date-parts":[[2002,7,23]]},"page":"694–699","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Transforming classifier scores into accurate multiclass probability estimates","type":"paper-conference","URL":"http://doi.org/10.1145/775047.775151"}
]
